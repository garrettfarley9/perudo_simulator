{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2eGEQAb9l64yREmGqu7OP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garrettfarley9/perudo_simulator/blob/main/Perudo_math_sim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxkLkWdAE3kU",
        "outputId": "892ff8ee-4832-4756-eba6-36a1d260cd56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.02\n"
          ]
        }
      ],
      "source": [
        "from random import randint\n",
        "outcome = {1:0, 2:0, 3:0, 4:0, 5:0, 6:0} #map of die:freq\n",
        "sim =10000\n",
        "dice = 15\n",
        "counter = 0\n",
        "\n",
        "for simulations in range(sim):\n",
        "    for die in range(dice):\n",
        "      first_dice_roll = randint(1,6)\n",
        "      outcome[first_dice_roll] += 1\n",
        "    for key in outcome.keys():\n",
        "      if key >= 5:\n",
        "        counter += 1\n",
        "print(counter / sim * .01)\n",
        "# for key in outcome.keys():\n",
        "#   print(\"Freq of %s is: %s\"%(key, (outcome[key] + outcome.get(1))/sim*0.01))\n",
        "\n",
        "\n",
        "# for key in outcome.keys():\n",
        "#     print(\"Percentage for rolling a sum of %s is: %s\"%(key,outcome[key]/sim))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import factorial\n",
        "ndice = 20\n",
        "m = 5\n",
        "n = 3\n",
        "prob = 0\n",
        "\n",
        "if (ndice < m + n):\n",
        "   prob = 0\n",
        "else:\n",
        "  prob = factorial(ndice - m)/((factorial(n))*factorial(ndice-n-m)) * factorial(ndice)/(factorial(n)*factorial(ndice-n)) * (1/6)**(n+m) * (4/6)**(ndice-n-m)\n",
        "print(prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLXEWHX3O2i7",
        "outputId": "7d894734-21e5-491a-b818-8a8af0a996da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# trial = np.random.Generator.binomial(n = 15, p = .3333, size = 1)\n",
        "# print(trial)\n",
        "\n",
        "rng = np.random.default_rng()\n",
        "n, p = 10, .3333  # number of dice not in hand, probability of each number\n",
        "s = sum(rng.binomial(n, p, 1000)/1000)\n",
        "print(s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXnNn_cSaDGb",
        "outputId": "24457fd2-571c-4345-cb01-ef69bac5736f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.383999999999969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Player class\n",
        "class Player:\n",
        "  palafico = False\n",
        "  num_dice = 5\n",
        "  round_num = 0\n",
        "  def __init__(self):\n",
        "    self.dice = [0,0,0,0,0]\n",
        "  def __str__(self):\n",
        "    return str(self.dice)\n",
        "\n",
        "## Getters and Setters ##\n",
        "  def get_num_dice(self):\n",
        "    return self.num_dice\n",
        "  def set_num_dice(self, x):\n",
        "    self.num_dice = x\n",
        "  def toggle_palafico(self):\n",
        "    self.palafico = not self.palafico\n",
        "  def get_round_num(self):\n",
        "    return self.round_num\n",
        "\n",
        "##Info Seeking##\n",
        "  def is_palafico(self):\n",
        "    return str(self.palafico)\n",
        "  def is_dice(self):\n",
        "    return int(self.num_dice)\n",
        "  def is_round(self):\n",
        "    return int(self.round_num)\n",
        "\n",
        "##Sensing##\n",
        "  def is_total(self, players = 1):\n",
        "    return (players * 5) - (self.round_num -1)\n",
        "\n",
        "##Rounds##\n",
        "  def start_round(self):\n",
        "    for x in range(self.num_dice):\n",
        "      self.dice[x] = randint(1,6)\n",
        "    self.round_num += 1\n",
        "  def lose_dice(self):\n",
        "    self.dice.pop()\n",
        "    self.num_dice -= 1\n",
        "    if self.num_dice == 1:\n",
        "      self.toggle_palafico()\n",
        "\n",
        "##Actions##\n",
        "  def action(self, choice)\n",
        "    if choice == 0:\n",
        "      dudo()\n",
        "    else:\n",
        "      bid()\n",
        "\n",
        "me = Player()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "U9Dpp7o77f6j",
        "outputId": "c9a18523-1e19-4efd-ec6f-6b35e9db5564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0]\n",
            "[1, 1, 3, 3, 4]\n",
            "[1, 1, 3, 3]\n",
            "4\n",
            "[1, 1, 3]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'True'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Game(num_players = 0):\n",
        "  def __init__(self):\n",
        "    for player in range(num_players):\n",
        "      player\n"
      ],
      "metadata": {
        "id": "e1DiyEMwIrUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binomial Formula\n",
        "P(X = k) = (n choose k) * p^k * (1-p)^(n-k)\n",
        "\n",
        "VARIABLE SYMBOL VALUE FOR\n",
        "TEACHER’S PROBLEM\n",
        "Number of trials n  = 15\n",
        "Number of successes wanted k  = 5\n",
        "Probability of Success on one trial p  = 1/6\n",
        "Probability of Failure on one trial q = 1 - p 5/6\n",
        "\n",
        "P (Paul is correct) = P ( 5 ones in 15 dice ) =\n",
        "(15 choose 5) * (1/6)^5 * (5/6)^15-5= ... [calculator] ... ≈ 0.06"
      ],
      "metadata": {
        "id": "Y6RWnV5XPiDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def odds(n, k, p):\n",
        "  return math.comb(n,k) * (p)**k * (1-p)**(n-k)"
      ],
      "metadata": {
        "id": "gI8pFRwIQgi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "odds(n = 15, k = 5, p = (1/6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK1rNNRTRBTq",
        "outputId": "23011ec2-4a8d-4e70-c561-506885d2f39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06237156191077761"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reinforcement environment\n",
        "Q_new(s_t, a_t) = (1-alpha) * Q(s_t, a_t) + alpha * (r_t + g * maxQ(s_t+1, a))\n",
        "where alpha is learning rate\n",
        "g is discount rate\n",
        "r is reward  "
      ],
      "metadata": {
        "id": "91e60wJWwrhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up Q table\n",
        "#q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "#Used to set up unique environment q_table\n",
        "if start_q_table is None:\n",
        "    # initialize the q-table#\n",
        "    q_table = {}\n",
        "    for i in range(-SIZE+1, SIZE):\n",
        "        for ii in range(-SIZE+1, SIZE):\n",
        "            for iii in range(-SIZE+1, SIZE):\n",
        "                    for iiii in range(-SIZE+1, SIZE):\n",
        "                        q_table[((i, ii), (iii, iiii))] = [np.random.uniform(-5, 0) for i in range(4)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  # not a constant, qoing to be decayed\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# Decaying is being done every episode if episode number is within decaying range\n",
        "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "        epsilon -= epsilon_decay_value"
      ],
      "metadata": {
        "id": "jWcfPL_fwu6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q Learning for blackjack"
      ],
      "metadata": {
        "id": "KXHdhCAl3shc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "class const():\n",
        "###Class that contains various constants/parameters for the current problem\n",
        "###\n",
        "  def __init__(self):\n",
        "    self.gamma = 0.5\n",
        "    self.input_filename = 'random_policy_runs_mapping_2.csv'\n",
        "    self.output_filename = 'QLearning_policy_mapping_2.policy'\n",
        "    self.n_states = 183 # 21 for state mapping 1, 183 for state mapping 2\n",
        "    self.n_action = 2\n",
        "    self.alpha = 0.01\n",
        "    self.lambda_ = 0.1\n",
        "def update_q_lambda(Q_sa, N_sa, df_i, CONST):\n",
        "###Note: not used in final implementation\n",
        "\n",
        "# Update visit count\n",
        "    #N_sa[df_i.s][df_i.a] += 1\n",
        "# Temporal difference residue\n",
        "  diff = df_i.r + (CONST.gamma * max(Q_sa[df_i.sp])) - Q_sa[df_i.s][df_i.a]\n",
        "# Update action value function\n",
        "  Q_sa += (CONST.alpha * diff * N_sa)\n",
        "# Decay visit count\n",
        "  N_sa *= CONST.gamma * CONST.lambda_\n",
        "  return\n",
        "def train_q_lambda(input_file, CONST):\n",
        "###Note: not used in final implementation\n",
        "\n",
        "# Read in input datafile\n",
        "# df = pd.read_csv(input_file)\n",
        "\n",
        "# Initialize action value function to all zeros\n",
        "  Q_sa = np.zeros((CONST.n_states, CONST.n_action))\n",
        "# Initialize counter function\n",
        "  N_sa = np.zeros((CONST.n_states, CONST.n_action))\n",
        "# Iterate through each sample in datafile\n",
        "  for i in range(len(df)):\n",
        "    df_i = df.loc[i]\n",
        "    update_q_lambda(Q_sa, N_sa, df_i, CONST)\n",
        "# Policy is the index of the max value for each row in Q_sa\n",
        "  policy = np.argmax(Q_sa, axis=1)\n",
        "# Write policy to file\n",
        "  write_outfile(policy, CONST)\n",
        "  return\n",
        "def update_q_learning(Q_sa, df_i, CONST):\n",
        "###Perform Q-Learning update to action value function for a single sample\n",
        "\n",
        "# Temporal difference residue\n",
        "  diff = df_i.r + (CONST.gamma * max(Q_sa[df_i.sp])) - Q_sa[df_i.s][df_i.a]\n",
        "# Update action value function\n",
        "  Q_sa[df_i.s][df_i.a] += CONST.alpha * diff\n",
        "  return\n",
        "def train_q(input_file, CONST):\n",
        "###Train a policy using Q-learning algorithm and input datafile containing sample data\n",
        "\n",
        "# Read in input datafile\n",
        "  df = pd.read_csv(input_file)\n",
        "# Initialize action value function to all zeros\n",
        "  Q_sa = np.zeros((CONST.n_states, CONST.n_action))\n",
        "# Iterate through each sample in datafile\n",
        "  for i in range(len(df)):\n",
        "  df_i = df.loc[i]\n",
        "  update_q_learning(Q_sa, df_i, CONST)\n",
        "# Policy is the index of the max value for each row in Q_sa\n",
        "  policy = np.argmax(Q_sa, axis=1)\n",
        "# Write policy to file\n",
        "  write_outfile(policy, CONST)\n",
        "  return\n",
        "\n",
        "def write_outfile(policy, CONST):\n",
        "###Write policy to a .policy output file\n",
        "\n",
        "# Get output file name and path\n",
        "  output_dir = os.getcwd()\n",
        "  output_file = os.path.join(output_dir, f’{CONST.output_filename}’)\n",
        "# Open output file\n",
        "  df = open(output_file, ’w’)\n",
        "# Iterate through each value in policy, writing to output file\n",
        "  for i in range(CONST.n_states):\n",
        "    df.write(f’{policy[i]}\\n’)\n",
        "# Close output file\n",
        "  df.close()\n",
        "  return\n",
        "def main():\n",
        "  start = time.time()\n",
        "  CONST = const()\n",
        "  input_file = os.path.join(os.getcwd(), CONST.input_filename)\n",
        "  train_q(input_file, CONST)\n",
        "#train_q_lambda(input_file, CONST)\n",
        "  end = time.time()\n",
        "  print(f’Total time: {end-start:0.2f} seconds’)\n",
        "  print(f’Total time: {(end-start)/60:0.2f} minutes’)\n",
        "  return\n",
        "if __name__ == ’__main__’:\n",
        "main()\n"
      ],
      "metadata": {
        "id": "1qVQxBa-3vuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##SARSA Code##\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#~~~~~~~~~~~~~~~~~\n",
        "state_mapping = 1\n",
        "#~~~~~~~~~~~~~~~~~\n",
        "if state_mapping == 1:\n",
        "S = 21\n",
        "A = 2\n",
        "gam = 0.5\n",
        "alpha = 0.01\n",
        "maxIters = 5\n",
        "input_file = \"random_policy_runs_mapping_1.csv\"\n",
        "output_file = ’Sarsa_Policy_1.policy’\n",
        "elif state_mapping == 2:\n",
        "S = 183\n",
        "A = 2\n",
        "gam = 0.5\n",
        "alpha = 0.01\n",
        "maxIters = 5\n",
        "input_file = \"random_policy_runs_mapping_2.csv\"\n",
        "output_file = ’Sarsa_Policy_2.policy’\n",
        "df = pd.read_csv(input_file)\n",
        "s_data = df[’s’]\n",
        "a_data = df[’a’]\n",
        "r_data = df[’r’]\n",
        "sp_data = df[’sp’]\n",
        "# Initialize action value function to all zeros\n",
        "Q = np.zeros((S, A))\n",
        "for i in range(maxIters):\n",
        "print(i)\n",
        "for k in range(len(df)-1):\n",
        "s = s_data[k]\n",
        "a = a_data[k]\n",
        "r = r_data[k]\n",
        "sp = sp_data[k]\n",
        "ap = a_data[k+1]\n",
        "Q[s, a] = Q[s, a] + alpha*(r + gam*Q[sp, ap] - Q[s, a]) #Sarsa\n",
        "#Q[s, a] = Q[s, a] + alpha*(r + gam*max(Q[sp, :]) - Q[s, a]) #Q-learn\n",
        "15\n",
        "policy = np.argmax(Q, axis=1)\n",
        "# Get output file name and path\n",
        "output_dir = os.getcwd()\n",
        "output_file = os.path.join(output_dir, output_file)\n",
        "# Open output file\n",
        "DF = open(output_file, ’w’)\n",
        "# Iterate through each value in policy, writing to output file\n",
        "for i in range(S):\n",
        "DF.write(f’{policy[i]}\\n’)\n",
        "# Close output file\n",
        "DF.close()\n"
      ],
      "metadata": {
        "id": "S3nM6ZnIMyHF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}